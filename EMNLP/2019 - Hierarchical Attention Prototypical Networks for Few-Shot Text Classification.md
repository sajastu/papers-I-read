[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/named-entity-recognition-bc5cdr)](https://www.aclweb.org/anthology/D19-1045.pdf)  
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scibert-pretrained-contextualized-embeddings/named-entity-recognition-bc5cdr)](https://github.com/ChenRocks/fast_abs_rl)  

# Hierarchical Attention Prototypical Networks for Few-Shot Text Classification (EMNLP, 2019)
### Authors: Shengli Sun, Qingfeng Sun, Kevin Zhou, Tengchao Lv (Peking University, Microsoft)

- Supervised training data are few and difficult to be collected, and thus most of current classification models are not working well as they require a large amount of data.
- Proposing a hierarchical attention prototypical networks (HAPN) for few-shot text classification.
-  Designing the feature level, word level, and instance level multi cross attention for our model to enhance the expressive ability of semantic space.
- Evaluating on fewshot text classification datasets - FewRel and CSID, and achieve the state-of-the-art performance. 
- Visualization of hierarchical attention layers illustrates that our model can capture more important features, words, and instances separately.


## Introduction
- Few-shot learning has became an effective approach to solve
this challenge (low-resource setting), it can train a neural network with a
few parameters using few data but achieve good
performance.
- Prototypical networks (Snell et al., 2017), which
averages the vector of few support instances as the
class prototype and computes distance between
target query and each prototype, then classify the
query to the nearest prototypeâ€™s class
- Proposing a three-level hierachical attentive prototypical network for few-shot text classificaiton:
  - (1) *Feature-level attention:* CNNs to capture feature scores which differs for each class.
  - (2) *Word-level attention:* Using an attention mechanism for each word's hidden representation to learn their importance within an instance.
  - (3) *Instance-level multi cross attention:* with the help of multi cross attention between
support set and target query, the instance importance can be deterimind within a class. 
- __Contributions__
  - Proposing a hierarchical attention prototypical networks for few-shot text classification.
  - Achieving state-of-the-art performance on
FewRel and CSID datasets.
  - Faster and more extensible models.
  
## Method
The methodology is composed of 3 main parts:
  - *Instance Encoder*
  - *Prototypical Network*
  - *Hierachical Attention (HA)*
- *Instance Encoder*
  - Using CNNs to encode each instance within each class.
- *Prototypical Networks*
  - Simple, yet efficient idea. 
  - Producing a *Prototypical Vector* for each training class; the vector is an average over embedded instances (by instance encoder); it is kind of like a representative vector for each class that takes the instances into account.
  - Then given a query instance, the distance between its vector and the protopycal vector is computed. The nearest distance is the predicted class.
  - To compute distance, combining Euclidean distance and class feature score.
  
- *Hierachical Attention*
  - Feature-level attention: Using CNN netowrk to extract class features;  ![formula](https://render.githubusercontent.com/render/math?math=d(c_i%20,q^\prime)=(c_i%20-q^\prime)^2-\lambda_i) where ![formula](https://render.githubusercontent.com/render/math?math=\lambda_i) is the class feature score, and ![formula](https://render.githubusercontent.com/render/math?math=q^\prime) is the query instance passed through the word-level attention layer.
  - Word-level attention: to encode semantics and important words of an instance (which is unequal)... it works on the instance encoded vectors and finally results in ![formula](https://render.githubusercontent.com/render/math?math=s^j) which is word hidden representations weighted by attention score for a given (query) instance.
   - Instance-level multi cross attention (the contribution of this paper): introduce the idea of assinging more weights to the instances that are contributive the query instance. As such, prototypical networks are just averaging, but here we should get the prototypical network by summing over the computed weights. Some math is injected... refer to the paper.
   
 
## Results
  - The HAPN could achieve highest accuracies on two experimental datasets. 
  - Hierachical Attention effect: 
  <p align="center"><img src="https://github.com/sajastu/papers-I-read/blob/master/EMNLP/pictures/HAPN-1.png" width="550"></p>
  - Although "mother" and "child" class are similar to each other, but the HAPN model can effectively classify the query instance to "mother" class.
      <p align="center"><img src="https://github.com/sajastu/papers-I-read/blob/master/EMNLP/pictures/HAPN-2.png" width="300"></p>
      - Same features extracted by feature-level attention (CNN) have different weights for different classes.
    

  
   
